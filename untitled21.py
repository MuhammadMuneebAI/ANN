# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PodviUUpPnkJStwfUwkGQEh3NO4o40Nx
"""

#1.Dataset Handling
import pandas as pd
import numpy as np

data_path = 'Parkinson’s Disease Detection dataset.csv'
df = pd.read_csv(data_path)
print("Sample data:")
df.head()

#b.Perform data cleaning, preprocessing, and feature engineering.
print("Dataset shape:", df.shape)
print("Dataset columns:", df.columns.tolist())
print("\nMissing values per column:")
print(df.isnull().sum())
#a.pick 3 for prediction

df.info()

df.describe()

df.shape

import torch
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X = df.drop(columns=['name', 'status'])
y = df['status']

X = X.values
y = y.values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y)

X_train_t = torch.tensor(X_train, dtype=torch.float32)
y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
X_test_t = torch.tensor(X_test, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

print(f'Train set: {X_train_t.shape}, {y_train_t.shape}')
print(f'Test set: {X_test_t.shape}, {y_test_t.shape}')

#2.Model Development
import torch.nn as nn
import torch.optim as optim

#b.Basic Perceptron Model
class Perceptron(nn.Module):
    def __init__(self, input_dim):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(input_dim, 1)
        self.sigmoid = nn.Sigmoid()  # for binary classification

    def forward(self, x):
        out = self.linear(x)
        out = self.sigmoid(out)
        return out

#f.Optimization Techniques: SGD
# Initialize model, loss, optimizer
input_dim = X_train_t.shape[1]
model = Perceptron(input_dim)

criterion = nn.BCELoss()  # binary cross-entropy loss
optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD optimizer

# Training loop
num_epochs = 100
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_t)
    loss = criterion(outputs, y_train_t)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

# Evaluation on test data
model.eval()
with torch.no_grad():
    preds = model(X_test_t)
    predicted = (preds >= 0.5).float()
    accuracy = (predicted == y_test_t).float().mean()
    print(f"Test Accuracy: {accuracy.item()*100:.2f}%")

#c.Multilayer Perceptron (MLP) for multi-disease prediction
class MLP_simple(nn.Module):
    def __init__(self, input_dim, hidden_dim=32, activation='relu', dropout_rate=0.1):
        super(MLP_simple, self).__init__()

        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            raise ValueError("Invalid activation function")

        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.layer2 = nn.Linear(hidden_dim, 1)
        self.sigmoid_out = nn.Sigmoid()

    def forward(self, x):
        x = self.layer1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.layer2(x)
        x = self.sigmoid_out(x)
        return x

# Training and evaluation function
def train_and_evaluate(model, optimizer, X_train, y_train, X_test, y_test, epochs=100):
    criterion = nn.BCELoss()
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

    model.eval()
    with torch.no_grad():
        preds = model(X_test)
        predicted = (preds >= 0.5).float()
        accuracy = (predicted == y_test).float().mean()
        print(f"Test Accuracy: {accuracy.item()*100:.2f}%")

#f.Optimization Techniques: Adam & SGD
# Initialize MLP with ReLU activation and Adam optimizer
mlp_simple = MLP_simple(input_dim=input_dim, hidden_dim=64, activation='relu', dropout_rate=0.1)
optimizer_sgd = optim.SGD(mlp_simple.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)

train_and_evaluate(mlp_simple, optimizer_sgd, X_train_t, y_train_t, X_test_t, y_test_t, epochs=100)

#Check class balance
import numpy as np

unique, counts = np.unique(y_train, return_counts=True)
print("Train label distribution:")
for u, c in zip(unique, counts):
    print(f"Class {u}: {c} samples")

unique_test, counts_test = np.unique(y_test, return_counts=True)
print("\nTest label distribution:")
for u, c in zip(unique_test, counts_test):
    print(f"Class {u}: {c} samples")

#d.Activation Functions: ReLU, sigmoid, tanh
# simple MLP with BCEWithLogitsLoss and weighted loss
class MLP_logits(nn.Module):
    def __init__(self, input_dim, hidden_dim=32, activation='relu', dropout_rate=0.1):
        super(MLP_logits, self).__init__()
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            raise ValueError("Invalid activation function")

        self.layer1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.layer2 = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.layer1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.layer2(x)
        return x

#e.Backpropagation Algorithm
# Training with weighted loss:
model_weighted = MLP_logits(input_dim=input_dim, hidden_dim=64, activation='relu', dropout_rate=0.1)
optimizer = optim.Adam(model_weighted.parameters(), lr=0.001, weight_decay=1e-4)

pos_weight = torch.tensor([counts[0] / counts[1]])
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

def train_weighted(model, optimizer, X_train, y_train, X_test, y_test, epochs=100):
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

    model.eval()
    with torch.no_grad():
        logits = model(X_test)
        preds = torch.sigmoid(logits)
        predicted = (preds >= 0.5).float()
        accuracy = (predicted == y_test).float().mean()
        print(f"Test Accuracy: {accuracy.item()*100:.2f}%")

train_weighted(model_weighted, optimizer, X_train_t, y_train_t, X_test_t, y_test_t, epochs=500)

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

#g.Regularization: L1, L2, Dropout, Batch Normalization
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

class DeepMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], activation='relu', dropout_rate=0.2, l1_reg=0.0, l2_reg=0.0, batch_norm=True):
        super(DeepMLP, self).__init__()

        # Activation function
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            raise ValueError("Invalid activation")

        # Layers
        layers = []
        last_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(last_dim, h_dim))
            layers.append(self.activation)
            layers.append(nn.Dropout(dropout_rate))  # Dropout layer
            if batch_norm:
                layers.append(nn.BatchNorm1d(h_dim))  # Batch Normalization
            last_dim = h_dim
        layers.append(nn.Linear(last_dim, 1))  # Output layer
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)
        self.l1_reg = l1_reg  # L1 regularization strength
        self.l2_reg = l2_reg  # L2 regularization strength

    def forward(self, x):
        return self.network(x)

    def regularization_loss(self):
        l1_loss = 0
        l2_loss = 0
        for param in self.parameters():
            l1_loss += torch.sum(torch.abs(param))
            l2_loss += torch.sum(param**2)
        return self.l1_reg * l1_loss + self.l2_reg * l2_loss

# Function to train and evaluate the model
def train_and_evaluate(model, optimizer, X_train, y_train, X_test, y_test, epochs=100):
    criterion = nn.BCELoss()
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)

        # Add regularization loss (L1, L2)
        loss += model.regularization_loss()

        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_test)
            val_loss = criterion(val_outputs, y_test)
            val_loss += model.regularization_loss()  # Validation loss with regularization
            val_losses.append(val_loss.item())

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}")

    # Plot training and validation loss
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Train and Validation Loss')
    plt.legend()
    plt.show()

    # Model evaluation
    model.eval()
    with torch.no_grad():
        preds = model(X_test)
        predicted = (preds >= 0.5).float()
        accuracy = (predicted == y_test).float().mean()
        print(f"Test Accuracy: {accuracy.item() * 100:.2f}%")

# Set up combinations of configurations
optimizers = ['sgd', 'adam']
dropout_rates = [0.1, 0.2, 0.3]
batch_norm_options = [True, False]
regularization_combos = [(0.0, 0.0), (0.001, 0.01), (0.01, 0.01)]  # (L1, L2)

# Initialize lists to store results
results = []

# Loop through all combinations
for optimizer_type in optimizers:
    for dropout_rate in dropout_rates:
        for batch_norm in batch_norm_options:
            for l1_reg, l2_reg in regularization_combos:
                print(f"Training with {optimizer_type.upper()} optimizer, Dropout={dropout_rate}, BatchNorm={batch_norm}, L1={l1_reg}, L2={l2_reg}")

                # Initialize model with current configuration
                model = DeepMLP(input_dim=X_train_t.shape[1], hidden_dims=[128, 64, 32], activation='relu',
                                 dropout_rate=dropout_rate, l1_reg=l1_reg, l2_reg=l2_reg, batch_norm=batch_norm)

                # Choose optimizer
                if optimizer_type == 'sgd':
                    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD with momentum
                else:
                    optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer

                # Train and evaluate the model
                train_and_evaluate(model, optimizer, X_train_t, y_train_t, X_test_t, y_test_t, epochs=100)

                # Store results for later comparison (you can modify this to store the loss/accuracy values)
                results.append({
                    'optimizer': optimizer_type,
                    'dropout_rate': dropout_rate,
                    'batch_norm': batch_norm,
                    'l1_reg': l1_reg,
                    'l2_reg': l2_reg
                })

# Print summary of experiments
print("Experiments completed.")

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import numpy as np


rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)


importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

print("Feature ranking:")
for f in range(X_train.shape[1]):
    print(f"{f + 1}. Feature {df.columns[indices[f]+1]} ({importances[indices[f]]:.4f})")  # +1 skips 'name'

# Plot top 10 features
plt.figure(figsize=(10,6))
plt.title("Feature importances")
plt.bar(range(10), importances[indices[:10]], color="r", align="center")
plt.xticks(range(10), [df.columns[i+1] for i in indices[:10]], rotation=45)
plt.xlim([-1, 10])
plt.show()

# Classification Report & ROC-AUC (using sklearn)
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt


y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:,1]

print(classification_report(y_test, y_pred))

roc_auc = roc_auc_score(y_test, y_proba)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.figure()
plt.plot(fpr, tpr, label=f"ROC curve (area = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic")
plt.legend(loc="lower right")
plt.show()

pip install optuna

import optuna

def objective(trial):
    # Hyperparameters to tune
    hidden_dim = trial.suggest_int('hidden_dim', 16, 128)
    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)
    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)
    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])

    model = MLP_simple(input_dim=input_dim, hidden_dim=hidden_dim, activation=activation, dropout_rate=dropout_rate)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()

    epochs = 50
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train_t)
        loss = criterion(outputs, y_train_t)
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        preds = model(X_test_t)
        predicted = (preds >= 0.5).float()
        accuracy = (predicted == y_test_t).float().mean()
    return accuracy.item()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)
print("Best trial:")
print(study.best_trial.params)

# deeper MLP
class DeepMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], activation='relu', dropout_rate=0.2):
        super(DeepMLP, self).__init__()

        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            raise ValueError("Invalid activation")

        layers = []
        last_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(last_dim, h_dim))
            layers.append(self.activation)
            layers.append(nn.Dropout(dropout_rate))
            last_dim = h_dim
        layers.append(nn.Linear(last_dim, 1))
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


def train_and_evaluate(model, optimizer, X_train, y_train, X_test, y_test, epochs=100):
    criterion = nn.BCELoss()
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

    model.eval()
    with torch.no_grad():
        preds = model(X_test)
        predicted = (preds >= 0.5).float()
        accuracy = (predicted == y_test).float().mean()
        print(f"Test Accuracy: {accuracy.item()*100:.2f}%")


deep_model = DeepMLP(input_dim=input_dim, hidden_dims=[128,64,32], activation='relu', dropout_rate=0.2)
optimizer = optim.Adam(deep_model.parameters(), lr=0.001)

train_and_evaluate(deep_model, optimizer, X_train_t, y_train_t, X_test_t, y_test_t, epochs=100)

from sklearn.feature_selection import SelectFromModel

# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Feature importance
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

print("Top 10 features by importance:")
for i in range(10):
    print(f"{i+1}. {df.columns[indices[i]+1]} ({importances[indices[i]]:.4f})")  # +1 skips 'name'

# Select top 10 features
selector = SelectFromModel(rf, prefit=True, max_features=10)
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

print(f"Original feature count: {X_train.shape[1]}, Selected feature count: {X_train_selected.shape[1]}")

# Convert to tensors
X_train_sel_t = torch.tensor(X_train_selected, dtype=torch.float32)
X_test_sel_t = torch.tensor(X_test_selected, dtype=torch.float32)

# Define and train MLP on selected features
mlp_selected = MLP_simple(input_dim=X_train_sel_t.shape[1], hidden_dim=64, activation='relu', dropout_rate=0.1)
optimizer_sel = optim.Adam(mlp_selected.parameters(), lr=0.001)

train_and_evaluate(mlp_selected, optimizer_sel, X_train_sel_t, y_train_t, X_test_sel_t, y_test_t, epochs=100)

# Deep MLP class (reuse from before)
class DeepMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], activation='relu', dropout_rate=0.2):
        super(DeepMLP, self).__init__()

        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            raise ValueError("Invalid activation")

        layers = []
        last_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(last_dim, h_dim))
            layers.append(self.activation)
            layers.append(nn.Dropout(dropout_rate))
            last_dim = h_dim
        layers.append(nn.Linear(last_dim, 1))
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Initialize model on selected features
deep_mlp_sel = DeepMLP(input_dim=X_train_sel_t.shape[1], hidden_dims=[128,64,32], activation='relu', dropout_rate=0.2)
optimizer_deep_sel = optim.Adam(deep_mlp_sel.parameters(), lr=0.001)

# Train & evaluate
train_and_evaluate(deep_mlp_sel, optimizer_deep_sel, X_train_sel_t, y_train_t, X_test_sel_t, y_test_t, epochs=100)

#3.Advanced Topics
#a.Self-Organizing Maps (SOM): For clustering patients into risk groups.

import numpy as np
import matplotlib.pyplot as plt

class SOM:
    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):
        """
        m x n is the size of SOM grid,
        dim is input feature dimension
        n_iterations - number of training iterations
        """
        self.m = m
        self.n = n
        self.dim = dim
        self.n_iterations = n_iterations
        self.alpha = alpha if alpha else 0.3  # initial learning rate
        self.sigma = sigma if sigma else max(m, n) / 2  # initial neighborhood radius

        # Initialize weights randomly
        self.weights = np.random.rand(m, n, dim)

        # Precompute coordinate grid
        self._neuron_locations = np.array(list(self._neuron_locations_generator()))

    def _neuron_locations_generator(self):
        for i in range(self.m):
            for j in range(self.n):
                yield np.array([i, j])

    def train(self, data):
        for it in range(self.n_iterations):
            # Decay functions
            lr = self.alpha * (1 - it / self.n_iterations)
            radius = self.sigma * (1 - it / self.n_iterations)

            # Pick a random sample
            sample = data[np.random.randint(0, data.shape[0])]

            # Find BMU (Best Matching Unit)
            bmu_idx = self._find_bmu(sample)

            # Update weights of neurons within the neighborhood radius
            for idx in range(self.m * self.n):
                neuron_loc = self._neuron_locations[idx]
                dist_to_bmu = np.linalg.norm(neuron_loc - bmu_idx)
                if dist_to_bmu <= radius:
                    influence = np.exp(-dist_to_bmu**2 / (2 * (radius**2)))
                    self.weights[neuron_loc[0], neuron_loc[1], :] += lr * influence * (sample - self.weights[neuron_loc[0], neuron_loc[1], :])

            if it % 10 == 0:
                print(f"Iteration {it}/{self.n_iterations}")

    def _find_bmu(self, sample):
        # Compute distances to all neurons
        diff = self.weights - sample.reshape(1, 1, self.dim)
        dist = np.linalg.norm(diff, axis=2)
        bmu_idx = np.unravel_index(np.argmin(dist, axis=None), dist.shape)
        return np.array(bmu_idx)

    def map_vects(self, data):
        bmu_indices = []
        for sample in data:
            bmu_indices.append(self._find_bmu(sample))
        return np.array(bmu_indices)

# Usage example:
data_np = X_train  # use your scaled data as numpy array
som = SOM(m=10, n=10, dim=data_np.shape[1], n_iterations=200)
som.train(data_np)

mapped = som.map_vects(data_np)

# Plotting BMU locations for data points colored by true class (status)
plt.figure(figsize=(8, 8))
for c in np.unique(y_train):
    idx = np.where(y_train == c)
    plt.scatter(mapped[idx,0], mapped[idx,1], label=f'Class {c}', alpha=0.6)
plt.title("SOM clustering of training data")
plt.legend()
plt.show()

#b.Radial Basis Function Network (RBFN): For anomaly detection (e.g., rare disease signs).
from sklearn.cluster import KMeans
import torch
import torch.nn as nn
import torch.nn.functional as F

class RBFN(nn.Module):
    def __init__(self, input_dim, num_centers):
        super(RBFN, self).__init__()
        self.num_centers = num_centers
        self.centers = nn.Parameter(torch.randn(num_centers, input_dim))
        self.log_sigma = nn.Parameter(torch.zeros(1))  # learnable width
        self.linear = nn.Linear(num_centers, 1)

    def kernel_function(self, x):
        # x shape: batch x input_dim
        # centers shape: num_centers x input_dim
        # output shape: batch x num_centers
        size = (x.size(0), self.num_centers, x.size(1))
        x_expanded = x.unsqueeze(1).expand(size)
        centers_expanded = self.centers.unsqueeze(0).expand(size)
        dist = (x_expanded - centers_expanded).pow(2).sum(2)
        sigma = torch.exp(self.log_sigma)
        return torch.exp(-dist / (2 * sigma ** 2))

    def forward(self, x):
        phi = self.kernel_function(x)
        out = self.linear(phi)
        return torch.sigmoid(out)

# Initialize RBF centers with KMeans
num_centers = 20
kmeans = KMeans(n_clusters=num_centers, random_state=42)
kmeans.fit(X_train)

rbfn = RBFN(input_dim=X_train.shape[1], num_centers=num_centers)
rbfn.centers.data = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)

# Train RBFN like other PyTorch models (using BCELoss)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(rbfn.parameters(), lr=0.01)

X_train_t = torch.tensor(X_train, dtype=torch.float32)
y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
X_test_t = torch.tensor(X_test, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

for epoch in range(100):
    rbfn.train()
    optimizer.zero_grad()
    outputs = rbfn(X_train_t)
    loss = criterion(outputs, y_train_t)
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

rbfn.eval()
with torch.no_grad():
    preds = rbfn(X_test_t)
    predicted = (preds >= 0.5).float()
    accuracy = (predicted == y_test_t).float().mean()
    print(f"RBFN Test Accuracy: {accuracy.item()*100:.2f}%")

#c.Bidirectional Associative Memory (BAM): For associating symptoms with diseases (as a rule-based system).
import numpy as np

class BAM:
    def __init__(self, input_size, output_size):
        self.W = np.zeros((input_size, output_size))

    def train(self, X, Y):
        # X: input patterns (binary +/-1), shape (num_samples, input_size)
        # Y: output patterns (binary +/-1), shape (num_samples, output_size)
        for x, y in zip(X, Y):
            self.W += np.outer(x, y)

    def recall(self, x, max_iter=10):
        y = np.sign(np.dot(x, self.W))
        for _ in range(max_iter):
            x_new = np.sign(np.dot(y, self.W.T))
            y_new = np.sign(np.dot(x_new, self.W))
            if np.array_equal(x, x_new) and np.array_equal(y, y_new):
                break
            x, y = x_new, y_new
        return x, y

# Example usage:
# Suppose symptoms and diseases are binary vectors encoded as +1/-1

# Prepare dummy binary data for demo
X_symptoms = np.array([[1, -1, 1, -1], [-1, 1, -1, 1]])
Y_diseases = np.array([[1, -1], [-1, 1]])

bam = BAM(input_size=4, output_size=2)
bam.train(X_symptoms, Y_diseases)

# Recall example
x_test = np.array([1, -1, 1, -1])
x_out, y_out = bam.recall(x_test)
print("Recalled symptom:", x_out)
print("Recalled disease:", y_out)

# Reshape data for LSTM: (batch_size, seq_len, input_size)
# Here: seq_len = number of features, input_size = 1

X_train_lstm = X_train_t.unsqueeze(2)  # shape: [800, 22, 1]
X_test_lstm = X_test_t.unsqueeze(2)    # shape: [200, 22, 1]

import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super(LSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x shape: (batch, seq_len, input_size)
        out, (h_n, c_n) = self.lstm(x)  # out shape: (batch, seq_len, hidden_size)
        # Take last timestep output
        out = out[:, -1, :]  # shape: (batch, hidden_size)
        out = self.fc(out)
        out = self.sigmoid(out)
        return out


class BiLSTMClassifier(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=1):
        super(BiLSTMClassifier, self).__init__()
        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers,
                              batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_size * 2, 1)  # *2 for bidirectional
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out, (h_n, c_n) = self.bilstm(x)
        out = out[:, -1, :]
        out = self.fc(out)
        out = self.sigmoid(out)
        return out

from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn

def train_evaluate_with_val(model, optimizer, X_train, y_train, X_val, y_val, epochs=50):
    criterion = nn.BCELoss()
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        loss = criterion(outputs, y_train)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)
            val_losses.append(val_loss.item())

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}")

    # Plot losses
    plt.figure(figsize=(10,5))
    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')
    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Train and Validation Loss')
    plt.legend()
    plt.show()

    # Evaluate on validation set for metrics and plots
    model.eval()
    with torch.no_grad():
        val_probs = model(X_val).squeeze().numpy()
        val_preds = (val_probs >= 0.5).astype(int)
        y_true = y_val.numpy()

    # Confusion Matrix
    cm = confusion_matrix(y_true, val_preds)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred 0','Pred 1'], yticklabels=['True 0','True 1'])
    plt.title('Confusion Matrix')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

    # Precision-Recall curve
    precision, recall, _ = precision_recall_curve(y_true, val_probs)
    ap_score = average_precision_score(y_true, val_probs)
    plt.figure(figsize=(8,6))
    plt.plot(recall, precision, label=f'Precision-Recall curve (AP={ap_score:.3f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    plt.grid()
    plt.show()

    accuracy = np.mean(val_preds == y_true)
    print(f'Validation Accuracy: {accuracy*100:.2f}%')

from sklearn.model_selection import train_test_split

X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(
    X_train_lstm, y_train_t, test_size=0.2, random_state=42, stratify=y_train_t)

print(f"Train shape: {X_train_part.shape}, Val shape: {X_val_part.shape}")

# Example with LSTM
lstm_model = LSTMClassifier(input_size=1, hidden_size=64)
optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.001)

train_evaluate_with_val(lstm_model, optimizer_lstm, X_train_part, y_train_part, X_val_part, y_val_part, epochs=50)

bilstm_model = BiLSTMClassifier(input_size=1, hidden_size=64)
optimizer_bilstm = torch.optim.Adam(bilstm_model.parameters(), lr=0.001)

train_evaluate_with_val(bilstm_model, optimizer_bilstm, X_train_part, y_train_part, X_val_part, y_val_part, epochs=50)

import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1. Select the top 10 features
selected_features = [
    'MDVP:Shimmer', 'Jitter:DDP', 'MDVP:Jitter(%)', 'MDVP:APQ', 'MDVP:Jitter(Abs)',
    'MDVP:PPQ', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:RAP', 'D2'
]

# Assuming df is your DataFrame containing the features
X_selected = df[selected_features]

# 2. Initialize StandardScaler
scaler = StandardScaler()

# 3. Fit the scaler on the selected features and transform them
X_selected_scaled = scaler.fit_transform(X_selected)

# Convert to a DataFrame with the same column names for easier access
X_selected_scaled_df = pd.DataFrame(X_selected_scaled, columns=selected_features)

# Print the scaled data
print("Scaled Selected Features:")
print(X_selected_scaled_df.head())

# 4. Save the fitted scaler for later use (for inference)
import joblib
joblib.dump(scaler, "scaler_selected_features.save")

# Deep MLP class (reuse from before)
class DeepMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], activation='relu', dropout_rate=0.2):
        super(DeepMLP, self).__init__()

        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'sigmoid':
            self.activation = nn.Sigmoid()
        elif activation == 'tanh':
            self.activation = nn.Tanh()
        else:
            raise ValueError("Invalid activation")

        layers = []
        last_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(last_dim, h_dim))
            layers.append(self.activation)
            layers.append(nn.Dropout(dropout_rate))
            last_dim = h_dim
        layers.append(nn.Linear(last_dim, 1))
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Initialize model on selected features
deep_mlp_sel = DeepMLP(input_dim=X_train_sel_t.shape[1], hidden_dims=[128,64,32], activation='relu', dropout_rate=0.2)
optimizer_deep_sel = optim.Adam(deep_mlp_sel.parameters(), lr=0.01)

# Train & evaluate
train_and_evaluate(deep_mlp_sel, optimizer_deep_sel, X_train_sel_t, y_train_t, X_test_sel_t, y_test_t, epochs=100)

# Save model weights
torch.save(deep_mlp_sel.state_dict(), "deep_mlp_selected_features.pth")
print("Model saved successfully.")

# Recreate the model architecture (must match training)
loaded_model = DeepMLP(
    input_dim=X_train_sel_t.shape[1],
    hidden_dims=[128, 64, 32],
    activation='relu',
    dropout_rate=0.2
)

# Load saved weights
loaded_model.load_state_dict(torch.load("deep_mlp_selected_features.pth"))
loaded_model.eval()
print("Model loaded and ready for inference.")

import torch
import pandas as pd
import numpy as np

# Assuming 'df' is the full dataset with all columns, and the scaler is already fitted
# Feature importance ranking
selected_features = [
    'MDVP:Shimmer', 'Jitter:DDP', 'MDVP:Jitter(%)', 'MDVP:APQ', 'MDVP:Jitter(Abs)',
    'MDVP:PPQ', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:RAP', 'D2'
]

# Select relevant features from the dataset
X_selected = df[selected_features]

# Randomly sample one test input
random_sample = X_selected.sample(n=1, random_state=42)
print("Random Test Input:", random_sample)

# Scale the test input using the same scaler
random_sample_scaled = scaler.transform(random_sample)

# Convert to tensor for inference
X_new_t = torch.tensor(random_sample_scaled, dtype=torch.float32)

# Load the trained model
loaded_model.eval()

# Run inference
with torch.no_grad():
    probs = loaded_model(X_new_t).squeeze().numpy()

# Convert probabilities to class labels (threshold 0.5)
pred_labels = (probs >= 0.5).astype(int)

# Display results
print("Predicted probabilities:", probs)
print("Predicted class labels:", pred_labels)

import torch
import numpy as np
import pandas as pd

# List of selected features
selected_features = [
    'MDVP:Shimmer', 'Jitter:DDP', 'MDVP:Jitter(%)', 'MDVP:APQ', 'MDVP:Jitter(Abs)',
    'MDVP:PPQ', 'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:RAP', 'D2'
]

# Collect input values from user
input_values = []
print("Please enter values for the following features:")

for feature in selected_features:
    while True:
        try:
            val = float(input(f"{feature}: "))
            input_values.append(val)
            break
        except ValueError:
            print("Invalid input. Please enter a numeric value.")

# Convert input list to numpy array and reshape for scaler
input_array = np.array(input_values).reshape(1, -1)

# Scale the input using the previously fitted scaler
input_scaled = scaler.transform(input_array)

# Convert to torch tensor
X_new_t = torch.tensor(input_scaled, dtype=torch.float32)

# Run inference
loaded_model.eval()
with torch.no_grad():
    probs = loaded_model(X_new_t).squeeze().numpy()

# Convert probability to class label
pred_labels = (probs >= 0.5).astype(int)

print("\nPrediction results:")
print(f"Predicted probability of positive class: {probs:.4f}")
print(f"Predicted class label: {pred_labels}")

#Data Visualization
import matplotlib.pyplot as plt
df.hist(figsize=(12, 10), bins=30)
plt.tight_layout()
plt.show()

corr = df.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

for col in df.select_dtypes(include=np.number).columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot - {col}')
    plt.show()

numerical_cols = df.select_dtypes(include=np.number).columns
for col in numerical_cols:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.show()

sns.countplot(x ='status', data = df)
plt.show()

outcome_counts = df['status'].value_counts()
labels = outcome_counts.index.map({0: 'Parkinson’s Disease Not Detected', 1: 'Parkinson’s Disease Detected'}).tolist()
fig = plt.figure(figsize=(10, 7))
plt.pie(outcome_counts.values, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Pie Chart Distribution')
plt.axis('equal')
plt.show()

if len(numerical_cols) >= 2:
    plt.figure(figsize=(6, 4))
    sns.scatterplot(data=df, x=numerical_cols[0], y=numerical_cols[1])
    plt.title(f'Scatter Plot: {numerical_cols[0]} vs {numerical_cols[1]}')
    plt.show()

if 'status' in df.columns:
    outcome_counts = df['status'].value_counts()
    plt.bar(outcome_counts.index.map({0: 'Parkinson’s Disease Not Detected', 1: 'Parkinson’s Disease Detected'}).tolist(), outcome_counts.values)
    plt.title('Bar Distribution')
    plt.xlabel('status')
    plt.ylabel('MDVP:Fo(Hz)')
    plt.show()
else:
    print("Error: 'Outcome' column not found in the DataFrame.")

sns.set(style = 'whitegrid')
sns.violinplot(x ="status",
             y ="D2",
             data = df)
plt.show()

sns.set(style='whitegrid')
sns.swarmplot(x="MDVP:Shimmer",
                  y="MDVP:Shimmer(dB)",
                  data=df)
plt.show()

sns.pairplot(df.select_dtypes(include=np.number))
plt.show()